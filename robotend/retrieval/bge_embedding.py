from typing import List, Tuple
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from retrieval.base import Retrieval


class BgeEmbedding(Retrieval):

    def __init__(self, embedding_name_or_path: str) -> None:
        super().__init__(embedding_name_or_path)

        self.tokenizer = AutoTokenizer.from_pretrained(
            self.embedding_name_or_path)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            self.embedding_name_or_path)
        self.model.eval()

    @torch.no_grad()
    def compute_score(self, pair: Tuple[str]) -> float:
        inputs = self.tokenizer(pair,
                                padding=True,
                                truncation=True,
                                return_tensors='pt',
                                max_length=512)

        scores = self.model(**inputs,
                            return_dict=True).logits.view(-1, ).float()
        return scores

    @torch.no_grad
    def compute_scores(self, pairs: List[Tuple[str, str]]) -> List[float]:
        all_score = []
        for pair in pairs:
            all_score.append(self.compute_score(pair))
        return all_score
