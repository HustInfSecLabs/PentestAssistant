from dotenv import dotenv_values
from semantic_kernel.connectors.ai.open_ai import (
    OpenAITextCompletion,
    AzureTextCompletion,
    OpenAIChatCompletion,
    AzureChatCompletion,
)
from semantic_kernel.connectors.ai.hugging_face import HuggingFaceTextCompletion
from semantic_kernel.kernel import Kernel

from config.connector import Llama70bGGUFCompletion


def add_completion_service(kernel: Kernel):
    config = dotenv_values(".env")
    llm_service = config.get("GLOBAL__LLM_SERVICE", None)
    model_load_type = config.get("GLOBAL__LLM_CALL_TYPE", None)

    if model_load_type is None:
        raise ValueError("The GLOBAL__LLM_CALL_TYPE value cannot be `None`")

    if model_load_type == "api":
        if llm_service == "AzureOpenAI":
            deployment_type = config.get("AZURE_OPEN_AI__DEPLOYMENT_TYPE",
                                         None)

            if deployment_type == "chat-completion":
                kernel.add_chat_service(
                    "chat_completion",
                    AzureChatCompletion(
                        config.get(
                            "AZURE_OPEN_AI__CHAT_COMPLETION_DEPLOYMENT_NAME",
                            None),
                        config.get("AZURE_OPEN_AI__ENDPOINT", None),
                        config.get("AZURE_OPEN_AI__API_KEY", None),
                    ),
                )
            else:
                kernel.add_text_completion_service(
                    "text_completion",
                    AzureTextCompletion(
                        config.get(
                            "AZURE_OPEN_AI__TEXT_COMPLETION_DEPLOYMENT_NAME",
                            None),
                        config.get("AZURE_OPEN_AI__ENDPOINT", None),
                        config.get("AZURE_OPEN_AI__API_KEY", None),
                    ),
                )
        elif llm_service == "OpenAI":
            model_id = config.get("OPEN_AI__MODEL_TYPE", None)

            if model_id == "chat-completion":
                kernel.add_chat_service(
                    "chat_completion",
                    OpenAIChatCompletion(
                        config.get("OPEN_AI__CHAT_COMPLETION_MODEL_ID", None),
                        config.get("OPEN_AI__API_KEY", None),
                        endpoint=config.get("OPEN_AI__API_BASE", None),
                    ),
                )
            else:
                kernel.add_text_completion_service(
                    "text_completion",
                    OpenAITextCompletion(
                        config.get("OPEN_AI__TEXT_COMPLETION_MODEL_ID", None),
                        config.get("OPEN_AI__API_KEY", None),
                        endpoint=config.get("OPEN_AI__API_BASE", None),
                    ),
                )
        elif llm_service == "THUDM/chatglm3-6b":
            model_id = config.get("ChatGLM3_6b_AI__OPEN_AI_MODEL_TYPE", None)
            if model_id == "chat-completion":
                kernel.add_chat_service(
                    "chat_completion",
                    OpenAIChatCompletion(
                        model_id="chatglm3-6b",
                        api_key="<API-KEY>",
                        endpoint=config.get(
                            "ChatGLM3_6b_AI__OPEN_AI_API_ENDPOINT", None)))
            else:
                kernel.add_text_completion_service(
                    "text_completion",
                    OpenAITextCompletion(
                        model_id="chatglm3-6b",
                        api_key="<API-KEY>",
                        endpoint=config.get(
                            "ChatGLM3_6b_AI__OPEN_AI_API_ENDPOINT", None)),
                )
        else:
            raise ValueError(
                "The LLM Call Type in Api not in [`AzureOpenAI`, `OpenAI`, `THUDM/chatglm3-6b`]"
            )
    elif model_load_type == "local":
        if llm_service == "THUDM/chatglm2-6b":
            model_id = llm_service
            model_device = config.get("ChatGLM2_6b_AI__MODEL_DEVICE", None)
            model_type = config.get("ChatGLM2_6b_AI__MODEL_TYPE", None)

            # if import model using local file, instead `model_id` using `model_path`
            model_path = config.get("ChatGLM2_6b_AI__MODEL_PATH", None)
            if model_path is not None:
                model_id = model_path

            kernel.add_text_completion_service(
                "text_completion",
                HuggingFaceTextCompletion(
                    model_id,
                    int(model_device),
                    model_type,
                    pipeline_kwargs={"trust_remote_code": True}))
        elif llm_service == "THUDM/chatglm3-6b":
            model_id = llm_service
            model_device = config.get("ChatGLM3_6b_AI__MODEL_DEVICE", None)
            model_type = config.get("ChatGLM3_6b_AI__MODEL_TYPE", None)

            # if import model using local file, instead `model_id` using `model_path`
            model_path = config.get("ChatGLM3_6b_AI__MODEL_PATH", None)
            if model_path is not None:
                model_id = model_path

            kernel.add_text_completion_service(
                "text_completion",
                HuggingFaceTextCompletion(
                    model_id,
                    int(model_device),
                    model_type,
                    pipeline_kwargs={"trust_remote_code": True}))
        elif llm_service == "TheBloke/Llama-2-70B-Chat-GGUF":
            kernel.add_text_completion_service("text_completion",
                                             Llama70bGGUFCompletion())
        elif llm_service == "meta-llama/Llama-2-7b-chat-hf":
            model_id = llm_service
            model_device = config.get("Llama2_6b_hf_AI__MODEL_DEVICE", None)
            model_type = config.get("Llama2_6b_hf_AI__MODEL_TYPE", None)

            # if import model using local file, instead `model_id` using `model_path`
            model_path = config.get("Llama2_6b_hf_AI__MODEL_PATH", None)
            if model_path is not None:
                model_id = model_path

            kernel.add_text_completion_service(
                "text_completion",
                HuggingFaceTextCompletion(model_id, int(model_device),
                                          model_type))
        elif llm_service == "meta-llama/Llama-2-13b-chat-hf":
            model_id = llm_service
            model_device = config.get("Llama2_13b_hf_AI__MODEL_DEVICE", None)
            model_type = config.get("Llama2_13b_hf_AI__MODEL_TYPE", None)

            # if import model using local file, instead `model_id` using `model_path`
            model_path = config.get("Llama2_13b_hf_AI__MODEL_PATH", None)
            if model_path is not None:
                model_id = model_path

            kernel.add_text_completion_service(
                "text_completion",
                HuggingFaceTextCompletion(model_id, int(model_device),
                                          model_type))
        elif llm_service == "meta-llama/Meta-Llama3-8b-instruct":
            import torch

            model_id = llm_service
            model_device = config.get("Llama3_8b_instruct_AI__MODEL_DEVICE",
                                      None)
            model_type = config.get("Llama3_8b_instruct_AI__MODEL_TYPE", None)

            # if import model using local file, instead `model_id` using `model_path`
            model_path = config.get("Llama3_8b_instruct_AI__MODEL_PATH", None)
            if model_path is not None:
                model_id = model_path

            kernel.add_text_completion_service(
                "text_completion",
                HuggingFaceTextCompletion(
                    model_id,
                    int(model_device),
                    model_type,
                    model_kwargs={"torch_dtype": torch.bfloat16}))
        else:
            raise ValueError(
                "The LLM Call Type in Api not in [`THUDM/chatglm2-6b`, `THUDM/chatglm3-6b`, `meta-llama/Llama-2-7b-chat-hf`]"
            )
    else:
        raise ValueError(
            "The GLOBAL__LLM_CALL_TYPE value only is in `[`local`, `api`]`")


# Kernel.add_completion_service = add_completion_service
